# @package model
# Nanda grokking setup: 1-layer transformer, 128 dim, 4 heads
_target_: project.lit_causal_lm.LitCausalLM
vocab_size: 120    # 3 specials + 113 digits + 4 ops
d_model: 128
n_layers: 1
n_heads: 4
d_mlp: 512
max_seq_len: 16
dropout: 0.0
tie_embed: false
lr: 1e-3
weight_decay: 1.0
warmup_steps: 10
