# @package model
# Nanda grokking setup: 1-layer, no LN, ReLU, untied embeddings
_target_: project.lit_modules.lit_causal_lm.LitCausalLM
vocab_size: 120    # 3 specials + 113 digits + 4 ops
d_model: 128
n_layers: 1
n_heads: 4
d_mlp: 512
max_seq_len: 8
dropout: 0.0
activation: relu
tie_embed: false
use_ln: false
lr: 1e-3
weight_decay: 1.0
betas: [0.9, 0.98]
warmup_steps: 10
