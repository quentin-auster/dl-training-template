vocab_size: 120
d_model: 128
n_layers: 1
n_heads: 4
d_mlp: 512
max_seq_len: 8
dropout: 0.0
activation: relu
tie_embed: false
use_ln: false
lr: 0.001
weight_decay: 1.0
betas:
- 0.9
- 0.98
warmup_steps: 10
log_every_n_epochs_phase1: 10
log_every_n_epochs_phase2: 100
log_phase_boundary: 100
modulus: 113
frac_train: 0.3
answer_only_supervision: true
include_bos: false
include_eos: false
use_plus: false
batch_size: 1024
num_workers: 0
seed: 0
